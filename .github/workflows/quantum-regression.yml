name: Quantum Regression Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  quantum-matrix:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.11', '3.12']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install wheel  # Ensure wheel is installed for faster installs
        # On Windows, install with reduced dependencies to avoid installation issues
        if [ "${{ runner.os }}" == "Windows" ]; then
          pip install -e ".[dev]"
        else
          pip install -e ".[dev,viz]"
        fi

    - name: Install platform-specific dependencies
      run: |
        if [ "${{ matrix.os }}" == "macos-latest" ]; then
          pip install -e ".[apple]" || echo "Apple dependencies not available"
        elif [ "${{ matrix.os }}" == "ubuntu-latest" ]; then
          pip install -e ".[cuda]" || echo "CUDA dependencies not available"
        else
          echo "No platform-specific dependencies for ${{ matrix.os }}"
        fi
      shell: bash

    - name: Run quantum regression tests
      run: python scripts/quantum_regression_test.py

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: quantum-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          benchmark_results.json
          success_rate.txt
        retention-days: 30

  cross-platform-summary:
    runs-on: ubuntu-latest
    needs: quantum-matrix
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/

    - name: Generate summary report
      shell: bash
      run: |
        python << 'EOF'
        import json
        import os
        from pathlib import Path

        print("Quantum Regression Test Summary")
        print("=" * 50)

        artifacts_dir = Path('artifacts')
        all_results = {}

        if not artifacts_dir.exists():
            print("No artifacts found - tests may have failed to run")
            exit(1)

        # Collect results from all matrix jobs
        for artifact_dir in artifacts_dir.iterdir():
            if artifact_dir.is_dir():
                benchmark_file = artifact_dir / 'benchmark_results.json'
                if benchmark_file.exists():
                    try:
                        with open(benchmark_file) as f:
                            results = json.load(f)
                            all_results[artifact_dir.name] = results
                    except Exception as e:
                        print(f"Error reading {benchmark_file}: {e}")

        if not all_results:
            print("No valid results found")
            exit(1)

        # Generate summary
        total_tests = 0
        total_successful = 0

        for job_name, results in all_results.items():
            print(f"\nJob: {job_name}")
            print("-" * 30)

            job_tests = 0
            job_successful = 0

            for alg_name, alg_data in results['results'].items():
                for backend_name, backend_data in alg_data['backends'].items():
                    job_tests += 1
                    total_tests += 1

                    if backend_data['success']:
                        job_successful += 1
                        total_successful += 1
                        print(f"  ✓ {alg_name} on {backend_name}")
                    else:
                        print(f"  ✗ {alg_name} on {backend_name}: {backend_data.get('error', 'Unknown')}")

            success_rate = job_successful / job_tests if job_tests > 0 else 0
            print(f"  Success rate: {success_rate:.2%} ({job_successful}/{job_tests})")

        # Overall summary
        overall_success_rate = total_successful / total_tests if total_tests > 0 else 0
        print(f"\nOVERALL SUMMARY")
        print("=" * 30)
        print(f"Total tests: {total_tests}")
        print(f"Successful: {total_successful}")
        print(f"Success rate: {overall_success_rate:.2%}")

        # Save summary
        summary = {
            'total_tests': total_tests,
            'successful_tests': total_successful,
            'success_rate': overall_success_rate,
            'job_results': {job: {'tests': len([b for a in r['results'].values() for b in a['backends'].values()]),
                                  'successful': len([b for a in r['results'].values() for b in a['backends'].values() if b['success']])}
                           for job, r in all_results.items()}
        }

        with open('summary_report.json', 'w') as f:
            json.dump(summary, f, indent=2)

        print(f"\n✓ Quantum regression tests completed with {overall_success_rate:.2%} success rate")
        EOF

    - name: Upload summary report
      uses: actions/upload-artifact@v4
      with:
        name: quantum-regression-summary
        path: summary_report.json
        retention-days: 90

  performance-trends:
    runs-on: ubuntu-latest
    needs: quantum-matrix
    if: github.event_name == 'schedule' || github.event_name == 'push'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,viz]"

    - name: Run performance benchmark
      shell: bash
      run: |
        python << 'EOF'
        import json
        import time
        from datetime import datetime

        try:
            from ariadne.benchmarking import export_benchmark_report

            print("Running performance trend benchmark...")

            # Run simple benchmark
            report = export_benchmark_report(
                algorithms=['bell', 'ghz'],
                backends=['auto', 'stim', 'qiskit'],
                shots=100,  # Reduced for CI
                fmt='json'
            )

            # Save with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'performance_trend_{timestamp}.json'

            with open(filename, 'w') as f:
                json.dump(report, f, indent=2, default=str)

            print(f"Performance data saved to: {filename}")

        except Exception as e:
            print(f"Performance benchmark failed: {e}")
            # Create minimal file to avoid upload errors
            with open('performance_trend_minimal.json', 'w') as f:
                json.dump({'error': str(e), 'timestamp': datetime.now().isoformat()}, f)
        EOF

    - name: Upload performance data
      uses: actions/upload-artifact@v4
      with:
        name: performance-trends
        path: performance_trend_*.json
        retention-days: 365
