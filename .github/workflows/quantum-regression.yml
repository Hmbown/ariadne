name: Quantum Regression Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  quantum-matrix:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.11', '3.12']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,viz]"

    - name: Install platform-specific dependencies
      shell: bash
      run: |
        if [[ "${{ matrix.os }}" == "macos-latest" ]]; then
          pip install -e ".[apple]" || echo "Apple dependencies not available"
        fi
        if [[ "${{ matrix.os }}" == "ubuntu-latest" ]]; then
          pip install -e ".[cuda]" || echo "CUDA dependencies not available"
        fi

    - name: Run quantum regression tests
      shell: bash
      run: |
        python << 'EOF'
        import json
        import time
        from datetime import datetime

        try:
            from ariadne import simulate, get_available_backends, explain_routing
            from ariadne.benchmarking import export_benchmark_report
            from qiskit import QuantumCircuit

            print("üöÄ Quantum Regression Test Suite")
            print("=" * 50)

            # Get available backends
            backends = get_available_backends()
            print(f"Available backends: {backends}")

            # Test algorithms
            algorithms_to_test = ['bell', 'ghz']  # Simple set for CI
            results = {'results': {}}

            for alg_name in algorithms_to_test:
                print(f"\nTesting {alg_name} algorithm...")
                results['results'][alg_name] = {'backends': {}}

                # Create test circuit based on algorithm
                if alg_name == 'bell':
                    qc = QuantumCircuit(2, 2)
                    qc.h(0)
                    qc.cx(0, 1)
                    qc.measure_all()
                elif alg_name == 'ghz':
                    qc = QuantumCircuit(3, 3)
                    qc.h(0)
                    qc.cx(0, 1)
                    qc.cx(1, 2)
                    qc.measure_all()

                # Test with available backends
                backends_to_test = ['qiskit', 'stim'] if 'stim' in backends else ['qiskit']

                for backend_name in backends_to_test:
                    try:
                        start_time = time.time()
                        result = simulate(qc, shots=100, backend=backend_name)
                        execution_time = time.time() - start_time

                        results['results'][alg_name]['backends'][backend_name] = {
                            'success': True,
                            'execution_time': execution_time,
                            'throughput': 100 / execution_time,
                            'backend_used': str(result.backend_used)
                        }
                        print(f"  ‚úì {backend_name}: {execution_time:.3f}s")

                    except Exception as e:
                        results['results'][alg_name]['backends'][backend_name] = {
                            'success': False,
                            'error': str(e),
                            'execution_time': 0,
                            'throughput': 0
                        }
                        print(f"  ‚úó {backend_name}: {e}")

            # Save results
            with open('benchmark_results.json', 'w') as f:
                json.dump(results, f, indent=2, default=str)

            # Calculate success rate
            total_tests = sum(len(alg['backends']) for alg in results['results'].values())
            successful_tests = sum(sum(1 for b in alg['backends'].values() if b['success'])
                                 for alg in results['results'].values())
            success_rate = successful_tests / total_tests if total_tests > 0 else 0

            with open('success_rate.txt', 'w') as f:
                f.write(f"{success_rate:.2%}")

            print(f"\nOverall success rate: {success_rate:.2%} ({successful_tests}/{total_tests})")

            # Exit with success if we have basic functionality
            if success_rate >= 0.5:  # At least 50% success rate
                print("‚úÖ Quantum regression tests passed!")
                exit(0)
            else:
                print("‚ùå Quantum regression tests failed!")
                exit(1)

        except Exception as e:
            print(f"‚ùå Critical error in quantum regression tests: {e}")
            exit(1)
        EOF

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: quantum-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          benchmark_results.json
          success_rate.txt
        retention-days: 30

  cross-platform-summary:
    runs-on: ubuntu-latest
    needs: quantum-matrix
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/

    - name: Generate summary report
      shell: bash
      run: |
        python << 'EOF'
        import json
        import os
        from pathlib import Path

        print("Quantum Regression Test Summary")
        print("=" * 50)

        artifacts_dir = Path('artifacts')
        all_results = {}

        if not artifacts_dir.exists():
            print("No artifacts found - tests may have failed to run")
            exit(1)

        # Collect results from all matrix jobs
        for artifact_dir in artifacts_dir.iterdir():
            if artifact_dir.is_dir():
                benchmark_file = artifact_dir / 'benchmark_results.json'
                if benchmark_file.exists():
                    try:
                        with open(benchmark_file) as f:
                            results = json.load(f)
                            all_results[artifact_dir.name] = results
                    except Exception as e:
                        print(f"Error reading {benchmark_file}: {e}")

        if not all_results:
            print("No valid results found")
            exit(1)

        # Generate summary
        total_tests = 0
        total_successful = 0

        for job_name, results in all_results.items():
            print(f"\nJob: {job_name}")
            print("-" * 30)

            job_tests = 0
            job_successful = 0

            for alg_name, alg_data in results['results'].items():
                for backend_name, backend_data in alg_data['backends'].items():
                    job_tests += 1
                    total_tests += 1

                    if backend_data['success']:
                        job_successful += 1
                        total_successful += 1
                        print(f"  ‚úì {alg_name} on {backend_name}")
                    else:
                        print(f"  ‚úó {alg_name} on {backend_name}: {backend_data.get('error', 'Unknown')}")

            success_rate = job_successful / job_tests if job_tests > 0 else 0
            print(f"  Success rate: {success_rate:.2%} ({job_successful}/{job_tests})")

        # Overall summary
        overall_success_rate = total_successful / total_tests if total_tests > 0 else 0
        print(f"\nOVERALL SUMMARY")
        print("=" * 30)
        print(f"Total tests: {total_tests}")
        print(f"Successful: {total_successful}")
        print(f"Success rate: {overall_success_rate:.2%}")

        # Save summary
        summary = {
            'total_tests': total_tests,
            'successful_tests': total_successful,
            'success_rate': overall_success_rate,
            'job_results': {job: {'tests': len([b for a in r['results'].values() for b in a['backends'].values()]),
                                  'successful': len([b for a in r['results'].values() for b in a['backends'].values() if b['success']])}
                           for job, r in all_results.items()}
        }

        with open('summary_report.json', 'w') as f:
            json.dump(summary, f, indent=2)

        print(f"\n‚úì Quantum regression tests completed with {overall_success_rate:.2%} success rate")
        EOF

    - name: Upload summary report
      uses: actions/upload-artifact@v4
      with:
        name: quantum-regression-summary
        path: summary_report.json
        retention-days: 90

  performance-trends:
    runs-on: ubuntu-latest
    needs: quantum-matrix
    if: github.event_name == 'schedule' || github.event_name == 'push'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,viz]"

    - name: Run performance benchmark
      shell: bash
      run: |
        python << 'EOF'
        import json
        import time
        from datetime import datetime

        try:
            from ariadne.benchmarking import export_benchmark_report

            print("Running performance trend benchmark...")

            # Run simple benchmark
            report = export_benchmark_report(
                algorithms=['bell', 'ghz'],
                backends=['auto', 'stim', 'qiskit'],
                shots=100,  # Reduced for CI
                fmt='json'
            )

            # Save with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'performance_trend_{timestamp}.json'

            with open(filename, 'w') as f:
                json.dump(report, f, indent=2, default=str)

            print(f"Performance data saved to: {filename}")

        except Exception as e:
            print(f"Performance benchmark failed: {e}")
            # Create minimal file to avoid upload errors
            with open('performance_trend_minimal.json', 'w') as f:
                json.dump({'error': str(e), 'timestamp': datetime.now().isoformat()}, f)
        EOF

    - name: Upload performance data
      uses: actions/upload-artifact@v4
      with:
        name: performance-trends
        path: performance_trend_*.json
        retention-days: 365
