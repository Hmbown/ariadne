name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ published ]
  workflow_dispatch:
    inputs:
      run_cuda_tests:
        description: "Run Windows CUDA validation (installs CUDA toolchain)"
        type: boolean
        default: false
      run_performance_tests:
        description: "Run comprehensive performance benchmarks"
        type: boolean
        default: false

env:
  PYTHON_VERSION: "3.11"
  # Cache configuration for faster builds
  CACHE_VERSION: "v2"

permissions:
  contents: write
  security-events: write
  actions: read
  packages: write
  pages: write
  id-token: write

jobs:
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
      - name: Lint and Format Check
        run: |
          ruff check src/ tests/
          ruff format --check src/ tests/
      - name: Type check with mypy
        run: mypy src/ariadne/
      - name: Security scan with bandit
        continue-on-error: true
        run: bandit -r src/ariadne/ -f json -o bandit-report.json
      - name: Dependency security scan with safety
        continue-on-error: true
        run: safety scan --json --output safety-report.json
      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          if-no-files-found: ignore
          retention-days: 30

  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            python-version: "3.11"
            enable-metal: false
          - os: ubuntu-latest
            python-version: "3.12"
            enable-metal: false
          - os: macos-latest
            python-version: "3.11"
            enable-metal: true
          - os: windows-latest
            python-version: "3.11"
            enable-metal: false
    steps:
      - uses: actions/checkout@v5
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
      - name: Get pip cache directory
        id: pip-cache
        shell: bash
        run: echo "dir=$(python -m pip cache dir)" >> "$GITHUB_OUTPUT"
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ${{ steps.pip-cache.outputs.dir }}
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-
            ${{ runner.os }}-pip-
      - name: Install dependencies
        shell: bash
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,viz]"
      - name: Install Apple Silicon dependencies
        if: matrix.enable-metal == true
        run: |
          pip install -e ".[apple]"
          # Verify Metal
          python -c "import jax; print(jax.devices())"
      - name: Run tests
        env:
          ARIADNE_DISABLE_RESOURCE_CHECKS: "1"
        run: |
          pytest tests/ -v --tb=short -n auto --cov=src/ariadne --cov-report=xml --cov-fail-under=60 --junitxml=pytest.xml
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
        with:
          file: ./coverage.xml
          flags: ${{ runner.os }}-${{ matrix.python-version }}
          name: codecov-${{ runner.os }}-${{ matrix.python-version }}
          fail_ci_if_error: false
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ runner.os }}-${{ matrix.python-version }}
          path: |
            .coverage
            tests/results/
            pytest.xml
          if-no-files-found: ignore
          retention-days: 30
      - name: Cross-Platform Perf Check
        continue-on-error: true
        run: |
          python benchmarks/quick_check.py --platform ${{ runner.os }}
        if: matrix.python-version == '3.11' && runner.os == 'Linux'
      - name: Run pre-commit hooks
        if: runner.os == 'Linux' && matrix.python-version == '3.11'
        continue-on-error: true
        run: pre-commit run --all-files

  test-notebooks:
    name: Validate Examples & Notebooks
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11"]
    steps:
      - uses: actions/checkout@v5
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,viz]"
          pip install nbconvert ipykernel jupyter pytest
      - name: Validate Python Examples
        shell: bash
        env:
          ARIADNE_DISABLE_RESOURCE_CHECKS: "1"
        run: |
          set -euo pipefail
          scripts=(
            "examples/basic/quickstart.py"
            "examples/backend_systems_demo.py"
            "examples/segmented_demo.py"
          )
          for script in "${scripts[@]}"; do
            echo "Running $script"
            python "$script"
          done
      - name: Execute key notebooks
        shell: bash
        env:
          ARIADNE_DISABLE_RESOURCE_CHECKS: "1"
        run: |
          set -euo pipefail
          notebooks=(
            "examples/01_qasm3_roundtrip.ipynb"
            "examples/02_stim_qec.ipynb"
          )
          for notebook in "${notebooks[@]}"; do
            echo "Executing $notebook"
            jupyter nbconvert --to notebook --execute "$notebook" --ExecutePreprocessor.timeout=300 --ExecutePreprocessor.allow_errors=True
          done
      - name: Upload Example Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: example-outputs
          path: |
            examples/*.ipynb  # Executed versions
            examples/*.png    # Generated plots
          if-no-files-found: ignore

  windows-cuda:
    name: Windows CUDA Tests
    runs-on: windows-latest
    if: inputs.run_cuda_tests == true
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-python@v6
        with:
          python-version: 3.11
      - name: Install CUDA deps
        run: |
          choco install cuda  # Chocolatey for CUDA
          pip install -e ".[cuda,dev]"
      - name: Run CUDA tests
        run: pytest tests/test_cuda_backend.py -v

  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || inputs.run_performance_tests == true
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 0  # Get full history for comparison
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Get pip cache directory
        id: pip-cache
        shell: bash
        run: echo "dir=$(python -m pip cache dir)" >> "$GITHUB_OUTPUT"
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ${{ steps.pip-cache.outputs.dir }}
          key: ${{ runner.os }}-pip-benchmarks-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-benchmarks-
            ${{ runner.os }}-pip-
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,viz]"
          pip install numpy pandas matplotlib seaborn
      - name: Run performance benchmarks
        run: |
          python benchmarks/run_all_benchmarks.py --quick --output-json || true
      - name: Download previous benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: previous-benchmarks/
        continue-on-error: true
      - name: Compare with previous results
        run: |
          python -c "
          import json
          import os
          from datetime import datetime

          # Load current results
          current_results = {}
          if os.path.exists('benchmarks/results/benchmark_summary.json'):
              with open('benchmarks/results/benchmark_summary.json', 'r') as f:
                  current_results = json.load(f)

          # Load previous results
          previous_results = {}
          if os.path.exists('previous-benchmarks/benchmark_summary.json'):
              with open('previous-benchmarks/benchmark_summary.json', 'r') as f:
                  previous_results = json.load(f)

          # Simple regression detection
          regressions = []
          for key, current_value in current_results.items():
              if key in previous_results and isinstance(current_value, (int, float)):
                  previous_value = previous_results[key]
                  if current_value > previous_value * 1.2:  # 20% slower
                      regressions.append(f'{key}: {previous_value:.3f}s -> {current_value:.3f}s')

          if regressions:
              print('⚠️ Performance regressions detected:')
              for regression in regressions:
                  print(f'  - {regression}')
              print('::warning::Performance regressions detected')
          else:
              print('✅ No significant performance regressions detected')
          "
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: benchmarks/results/
          retention-days: 90
      - name: Generate performance report
        if: always()
        run: |
          python -c "
          import json
          import os
          from datetime import datetime

          report = []
          report.append('# Performance Benchmark Report')
          report.append(f'Generated: {datetime.now().isoformat()}')
          report.append(f'Commit: {os.environ[\"GITHUB_SHA\"]}')
          report.append('')

          if os.path.exists('benchmarks/results/benchmark_summary.json'):
              with open('benchmarks/results/benchmark_summary.json', 'r') as f:
                  results = json.load(f)
              report.append('## Results')
              for key, value in results.items():
                  if isinstance(value, (int, float)):
                      report.append(f'- {key}: {value:.3f}s')
          else:
              report.append('No benchmark results available')

          with open('benchmarks/results/performance_report.md', 'w') as f:
              f.write('\\n'.join(report))
          "
      - name: Upload performance report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-report
          path: benchmarks/results/performance_report.md
          retention-days: 90

  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [code-quality, test]
    steps:
      - uses: actions/checkout@v5
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install build dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build twine
      - name: Build package
        run: python -m build
      - name: Check package
        run: twine check dist/*
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist
          path: dist/

  publish:
    name: Publish to PyPI
    runs-on: ubuntu-latest
    needs: [build]
    if: github.event_name == 'release' && github.event.action == 'published'
    permissions:
      id-token: write
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: dist
          path: dist/
      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1

  deploy-docs:
    name: Deploy Documentation
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: read
      pages: write
      id-token: write
    steps:
      - name: Checkout
        uses: actions/checkout@v5
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[docs,dev]"
      - name: Build documentation
        run: |
          sphinx-build -b html --keep-going docs/source docs/build/html
      - name: Setup Pages
        uses: actions/configure-pages@v4
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v4
        with:
          path: docs/build/html
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  reproducibility:
    name: Reproducibility Check
    runs-on: ubuntu-latest
    needs: [test]
    steps:
      - uses: actions/checkout@v5
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,viz]"
      - name: Reproducibility (GHZ-10 across qiskit/tensor_network/stim)
        shell: bash
        env:
          ARIADNE_DISABLE_RESOURCE_CHECKS: "1"
        run: |
          mkdir -p benchmarks/results
          python -m ariadne datasets list || true
          python -m ariadne repro \
            --circuit ghz_10 \
            --backends qiskit,tensor_network,stim \
            --shots 256 \
            --tolerance 0.10 \
            --pretty \
            --output benchmarks/results/repro_report.json \
            --export-csv benchmarks/results/repro_report.csv \
            --export-md benchmarks/results/repro_report.md \
            --export-html benchmarks/results/repro_report.html
      - name: Assert consistency
        run: |
          python - << 'PY'
          import json
          with open('benchmarks/results/repro_report.json') as f:
              data = json.load(f)
          assert 'consistent' in data, 'Missing key in report'
          if not data['consistent']:
              raise SystemExit('Cross-backend consistency check failed (see repro_report.json)')
          print('Reproducibility passed with max distance =', data['max_distance'])
          PY
      - name: Upload reproducibility artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: reproducibility-report
          path: benchmarks/results/
      - name: Reproducibility (QFT-10 across qiskit/tensor_network)
        shell: bash
        env:
          ARIADNE_DISABLE_RESOURCE_CHECKS: "1"
        run: |
          python -m ariadne repro \
            --circuit qft_10 \
            --backends qiskit,tensor_network \
            --shots 256 \
            --tolerance 0.20 \
            --pretty \
            --output benchmarks/results/repro_qft10.json \
            --export-csv benchmarks/results/repro_qft10.csv \
            --export-md benchmarks/results/repro_qft10.md \
            --export-html benchmarks/results/repro_qft10.html
      - name: Upload QFT reproducibility artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: reproducibility-qft10
          path: |
            benchmarks/results/repro_qft10.*
